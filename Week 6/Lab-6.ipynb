{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 6 : Artificial neural network in PyTorch \n",
    "```\n",
    "- Machine Learning, Innopolis University (Fall semester 2022)\n",
    "- Professor: Adil Khan\n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "In this lab, you will practice simple deep learning model in Pytorch.\n",
    "```\n",
    "Lab Plan\n",
    "1. Theoretical issues with ANNs\n",
    "2. Deep learning frameworks\n",
    "3. Introduction to Pytorch : Linear Regression with Pytorch\n",
    "3. Simple ANN model for classification\n",
    "4. Training ANNs\n",
    "```\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical issues\n",
    "Ordinary fully connected neural nets consists of Dense layers, activations, and output layer.\n",
    "\n",
    "1. What's the difference between deep learning and normal machine learning?\n",
    "2. How does a neural network with no hidden layers and one output neuron compare to a logistic/linear regression?\n",
    "3. Can the perceptron find a non-linear decision boundary?\n",
    "4. In multi-hidden layers network, what's the need of non-linear activation function?\n",
    "5. Is random weight assignment better than assigning same weights to the units in the hidden layer.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep learning framework : PyTorch\n",
    "\n",
    "Getting started with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Linear Regression with Numpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "def generate_data(size = 100):\n",
    "    x = np.random.rand(size, 1)\n",
    "    y = 3 + 2.5 * x + .1 * np.random.randn(size, 1)\n",
    "\n",
    "    # Shuffles the indices\n",
    "    idx = np.arange(size)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    # split to train and validation 80:20\n",
    "    split = int(size * 0.8)\n",
    "    train_idx = idx[:split]\n",
    "    val_idx = idx[split:]\n",
    "\n",
    "    # Generate train and validation sets\n",
    "    x_train, y_train = x[train_idx], y[train_idx]\n",
    "    x_val, y_val = x[val_idx], y[val_idx]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = generate_data()\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Title\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Descent algorithm\n",
    "\n",
    "Gradient descent consist of 3 basic steps :\n",
    "\n",
    "1. **Compute the Loss**\n",
    "\n",
    "$$ \\hat{y} = a + bx + \\epsilon $$\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{N} \\sum_{i} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{N} \\sum_{i} (y_i - a - bx_i)^2 $$\n",
    "\n",
    "2. **Compute the Gradients** : A gradient is a partial derivative. Using the chain rule the final expression came to be :\n",
    "\n",
    "$$\\frac{\\partial \\text{MSE}}{\\partial a} = \\frac{\\partial \\text{MSE}}{\\partial \\hat{y}} * \\frac{\\partial \\hat{y}}{\\partial a} = -2 * \\frac{1}{N} \\sum_{i} (y_i - \\hat{y}_i)$$\n",
    "\n",
    "$$\\frac{\\partial \\text{MSE}}{\\partial b} = \\frac{\\partial \\text{MSE}}{\\partial \\hat{y}} * \\frac{\\partial \\hat{y}}{\\partial b} = -2 * \\frac{1}{N} \\sum_{i} x_i(y_i - \\hat{y}_i)$$\n",
    "\n",
    "3. **Update the Parameters**\n",
    "\n",
    "$$a = a - \\alpha \\frac{\\partial \\text{MSE}}{\\partial a}$$\n",
    "\n",
    "$$b = b - \\alpha \\frac{\\partial \\text{MSE}}{\\partial b}$$\n",
    "\n",
    "4. Repeat step 1 to 3 till convergence is reached"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression model training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initializes parameters \"a\" and \"b\" randomly\n",
    "\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "print(f\"Initial values of [a, b] : [{a[0]}, {b[0]}]\")\n",
    "\n",
    "learning_rate = 1e-1 #learning rate\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1: Computes y hat\n",
    "    yhat = a + b * x_train\n",
    "\n",
    "    # Compute error and Loss using MSE\n",
    "    error = (y_train - yhat)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 2: Compute gradients for both \"a\" and \"b\" parameters (partial derivatives)\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "\n",
    "    # Step 3: Update parameters using gradients and the learning rate\n",
    "    a = a - learning_rate * a_grad\n",
    "    b = b - learning_rate * b_grad\n",
    "\n",
    "print(f\"Final values of [a, b] : [{a[0]}, {b[0]}]\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch basics\n",
    "\n",
    "### Tensors\n",
    "\n",
    "* How to create a Tensor\n",
    "* Operations on tensors\n",
    "* Data types for Tensors### Create a Tensor\n",
    "\n",
    "Create tensors from Numpy then see what operations can be applied.\n",
    "**Note:** By default a tensor resides in cpu but can be sent to the GPU for fatser computations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train).to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).to(device)\n",
    "\n",
    "# Here we can see the difference - notice that .type() is more useful\n",
    "# since it tells WHERE the tensor device\n",
    "\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression (Numpy -> PyTorch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = torch.randn(1,device=device, requires_grad=False)\n",
    "b = torch.randn(1, device=device, requires_grad=False)\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Step 1: Computes y hat\n",
    "    yhat = None\n",
    "\n",
    "    # Compute error and Loss using MSE\n",
    "    error = (y_train - yhat)\n",
    "    loss = None\n",
    "\n",
    "    # Step 2: Compute gradients for both \"a\" and \"b\" parameters (partial derivatives)\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "\n",
    "    # Step 3: Update parameters using gradients and the learning rate\n",
    "    a = a - learning_rate * a_grad\n",
    "    b = b - learning_rate * b_grad\n",
    "\n",
    "print(a, b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feed Forward Neural Network\n",
    "An artificial neural network wherein connections between the nodes do not form a cycle.\n",
    "<!--![alt text](https://upload.wikimedia.org/wikipedia/en/5/54/Feed_forward_neural_net.gif)\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/2294px-Artificial_neural_network.svg.png)-->\n",
    "\n",
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/2294px-Artificial_neural_network.svg.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Model Design in Pytorch\n",
    "we have three simple parts that we need to build:\n",
    "1. Data Loading process.\n",
    "2. Model building.\n",
    "3. the training loops.\n",
    "\n",
    "<strong>Data Loading</strong>\n",
    "\n",
    "Data Loading in pytorch is very easy and broken into 3 steps:\n",
    "1. Data Source\n",
    "2. Data Transformations\n",
    "3. Data Loader\n",
    "\n",
    "\n",
    "\n",
    "## 3. Loading data\n",
    "\n",
    "Pytorch uses data loading utility which is called `DataLoader` that supports:\n",
    "automatic batching, transformation, single- and multi-process data loading and more.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch. utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "test_batch_size = 100\n",
    "\n",
    "data_transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=data_transformations)\n",
    "mnist_test = datasets.MNIST('./data', train=False,\n",
    "                            transform=data_transformations)\n",
    "\n",
    "train_loader = DataLoader(mnist_train,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test,\n",
    "                         batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "images, labels = next(iter(train_loader))\n",
    "plt.imshow(images[0].reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model building\n",
    "1. Defining components: <br/>\n",
    "This step is done in the constructor, where you will define the layers that will be used accordingly in the next step.\n",
    "2. Network flow: <br/>\n",
    "This step is done in the forward function. Where you will get the input batch as an argument then you will use the defined layers in the previous step to define the flow of the network then you will return the output batch.\n",
    "\n",
    "\n",
    "Pytorch is a dynamic framework, where you can use primitive python keywords with it.\n",
    "You can use if and while statements. Also, it can accepts and returns more than one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)        \n",
    "        # Write 3 lines to define 3 more linear layers.\n",
    "        # 2 hidden layers with number of neurons numbers: 250 and 100\n",
    "        # 1 output layer that should output 10 neurons, one for each class.\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        self.fc4 = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # the linear layers fc1, fc2, fc3, and fc4\n",
    "        # accepts only flattened input (1D batches)\n",
    "        # while the batch x is of size (batch, 28 * 28)\n",
    "        # define one line to flatten the x to be of size (batch_sz, 28 * 28)\n",
    "        x = x.view(-1, 28*28)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training loops\n",
    "After that we should define the loops over tha batches and run the training on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training params\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Define the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = torch.nn.functional.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Define the evaluation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Do the same that was done in the previous function.\n",
    "            # But without backprobagating the loss and without running the optimizers\n",
    "            # As this function is only for test.\n",
    "            # write 3 lines to transform the data to the device, get the output and compute the loss\n",
    "            data, target = None, None\n",
    "            output = None\n",
    "            test_loss += torch.nn.functional.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Self-practice <center>\n",
    "\n",
    "Using Dataset from assignment 1\n",
    "1. Define, train and evaluate an ANN for Regression and Classification\n",
    "1. Plot the loss and accuracy of the model for each training iteration\n",
    "    \n",
    "ANN should be implemented in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
